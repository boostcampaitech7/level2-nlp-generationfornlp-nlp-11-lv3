peft_config:
  r: 12
  lora_alpha: 8
  lora_dropout: 0.05
  target_modules: [q_proj,k_proj]
  bias: none
  task_type: CAUSAL_LM

sft_config:
  do_train: True
  do_eval: True
  lr_scheduler_type: cosine
  max_seq_length: 1280
  output_dir: ../model_outputs
  batch_size: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  num_train_epochs: 3
  learning_rate: 2e-5
  weight_decay: 0.01
  logging_steps: 1
  save_strategy: epoch
  eval_strategy: epoch
  save_total_limit: 1
  save_only_model: True
  report_to: wandb


data:
  data_name: train.csv


